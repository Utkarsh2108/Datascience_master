{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q1`. What is Gradient Boosting Regression?\n"
      ],
      "metadata": {
        "id": "t1IRdYfDGeAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Regression is a machine learning algorithm that builds an ensemble of decision trees to predict a numerical target variable. It works by iteratively adding decision trees to the model, each one correcting the errors made by the previous trees. The algorithm optimizes the model by minimizing the residual errors using gradient descent. The final model is a weighted combination of the individual trees, where each tree's weight depends on its contribution to reducing the overall error. The result is a powerful predictive model that can handle complex non-linear relationships between the input features and the target variable."
      ],
      "metadata": {
        "id": "O6KSt0CFSyWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q2`. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
      ],
      "metadata": {
        "id": "CAWguzOdHnOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    \n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.residuals = []\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        # Initialize the residuals as the difference between the true values and the mean\n",
        "        self.residuals = y - np.mean(y)\n",
        "        # Fit a tree for each estimator\n",
        "        for i in range(self.n_estimators):\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, self.residuals)\n",
        "            # Predict the residuals and update them\n",
        "            residuals_pred = tree.predict(X)\n",
        "            self.residuals -= self.learning_rate * residuals_pred\n",
        "            # Add the tree to the list of trees\n",
        "            self.trees.append(tree)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        # Predict the residuals for each tree and sum them up\n",
        "        residuals_pred = np.sum(tree.predict(X) for tree in self.trees)\n",
        "        # Return the sum of the residuals and the mean\n",
        "        return np.mean(y) + self.learning_rate * residuals_pred\n",
        "\n",
        "# Generate some random data\n",
        "X = np.random.rand(100, 3)\n",
        "y = np.sum(X, axis=1) + np.random.normal(scale=0.1, size=100)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test = X[:80], X[80:]\n",
        "y_train, y_test = y[:80], y[80:]\n",
        "\n",
        "# Train the gradient boosting regressor on the training set\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate the model using mean squared error and R-squared\n",
        "mse = np.mean((y_pred - y_test)**2)\n",
        "r2 = 1 - mse / np.var(y_test)\n",
        "\n",
        "print(\"Mean squared error: {:.3f}\".format(mse))\n",
        "print(\"R-squared: {:.3f}\".format(r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jINCnhBiW9LK",
        "outputId": "76ea7273-1bf0-487d-e9af-3129ff3d4077"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 0.011\n",
            "R-squared: 0.949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q3`. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n"
      ],
      "metadata": {
        "id": "XQZHEtczHoku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "WuJYSNqTXhr5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer = load_breast_cancer()\n",
        "X,y = cancer.data , cancer.target"
      ],
      "metadata": {
        "id": "V7DjyYLUXygs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "m46u4glnYPOK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}"
      ],
      "metadata": {
        "id": "5K5AElCzYRjr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "Z5Qm8KiSYTXQ",
        "outputId": "ebb57731-38a8-4cd7-cd39-bf34a9928330"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=42),\n",
              "             n_jobs=-1,\n",
              "             param_grid={'learning_rate': [0.01, 0.1, 0.5],\n",
              "                         'max_depth': [3, 4, 5],\n",
              "                         'n_estimators': [50, 100, 150]},\n",
              "             scoring='neg_mean_squared_error')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=42),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.5],\n",
              "                         &#x27;max_depth&#x27;: [3, 4, 5],\n",
              "                         &#x27;n_estimators&#x27;: [50, 100, 150]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=42),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.5],\n",
              "                         &#x27;max_depth&#x27;: [3, 4, 5],\n",
              "                         &#x27;n_estimators&#x27;: [50, 100, 150]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best hyperparameters: \", grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVyPlLtFYWet",
        "outputId": "b560182e-d019-494e-f595-b855084f6c64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_search.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error: {:.3f}\".format(mse))\n",
        "print(\"R-squared: {:.3f}\".format(r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CIXUn2SYlZW",
        "outputId": "f13375b9-a81d-475a-cf8d-80b562e5d8ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 0.031\n",
            "R-squared: 0.866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q4`. What is a weak learner in Gradient Boosting?\n"
      ],
      "metadata": {
        "id": "-BTCrHETHptx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Gradient Boosting, a weak learner is a decision tree that has limited predictive power and is typically constructed with a small number of shallow nodes. The goal of the Gradient Boosting algorithm is to iteratively improve upon the weak learner by adding additional trees that correct the errors of the previous trees. The final model is a weighted sum of the individual trees, where each tree's weight is determined by its contribution to the overall prediction accuracy. The use of weak learners allows the algorithm to focus on the areas of the data that are difficult to predict, while avoiding overfitting to the training data."
      ],
      "metadata": {
        "id": "pgN7ZopcY1lV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q5`. What is the intuition behind the Gradient Boosting algorithm?\n"
      ],
      "metadata": {
        "id": "RVXmh04WHqt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intuition behind the Gradient Boosting algorithm is to create a strong predictive model by iteratively adding simple, weak models to the ensemble, with each new model trained to correct the errors of the previous ones.\n",
        "\n",
        "At each step, the algorithm calculates the gradient of the loss function with respect to the predicted values and uses this gradient to update the weights of the weak learners, which are then combined into a stronger ensemble model.\n",
        "\n",
        "This process continues until a stopping criterion is met, such as when the error on the validation set stops improving or when a maximum number of iterations is reached. The result is a powerful and flexible machine learning model that can handle complex relationships between the input features and the target variable, and that can generalize well to new data."
      ],
      "metadata": {
        "id": "7mi974StY3_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q6`. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
      ],
      "metadata": {
        "id": "3maORjLOHuEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. At each step of the algorithm, a new weak learner is added to the ensemble and its predictions are combined with the predictions of the previous weak learners to improve the overall prediction accuracy.\n",
        "\n",
        "The algorithm works as follows:\n",
        "\n",
        "- Initialize the ensemble with a constant value, such as the mean or median of the target variable.\n",
        "\n",
        "- Train a weak learner, such as a decision tree, on the training data. The weak learner is trained to minimize the loss function between the predicted values and the actual values.\n",
        "\n",
        "- Calculate the residuals between the predicted values and the actual values. These residuals represent the errors made by the current ensemble.\n",
        "\n",
        "- Train a new weak learner on the residuals. This new learner is trained to predict the residuals, so that it can correct the errors made by the previous learner.\n",
        "\n",
        "- Combine the predictions of the new learner with the predictions of the previous learners. This is done by adding the new learner's predictions to the predictions of the previous learners, with a weighting factor that determines the contribution of each learner to the final prediction.\n",
        "\n",
        "- Repeat steps 3 to 5 for a predetermined number of iterations or until a certain level of accuracy is achieved."
      ],
      "metadata": {
        "id": "X2DPyJngZHqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q7`. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
      ],
      "metadata": {
        "id": "7rhkKeP_HvUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "he mathematical intuition behind the Gradient Boosting algorithm can be broken down into the following steps:\n",
        "\n",
        "* Define a loss function that measures the difference between the predicted values and the true values of the target variable.\n",
        "\n",
        "* Initialize the model with a simple weak learner, such as a decision tree with few levels.\n",
        "\n",
        "* Calculate the negative gradient of the loss function with respect to the predicted values, which represents the direction and magnitude of the error.\n",
        "\n",
        "* Fit a new weak learner to the negative gradient, so that it corrects the errors made by the previous model.\n",
        "\n",
        "* Update the weights of the weak learners by applying a gradient descent optimization algorithm to minimize the loss function.\n",
        "\n",
        "* Combine the individual trees into an ensemble model by taking a weighted sum of their predictions, with each tree's weight determined by its contribution to reducing the overall error.\n",
        "\n",
        "* Repeat steps 3-6 for a fixed number of iterations or until the error on the validation set stops improving."
      ],
      "metadata": {
        "id": "0u3jHPMFeeA3"
      }
    }
  ]
}