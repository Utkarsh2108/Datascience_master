{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q1`. What is an ensemble technique in machine learning?\n"
      ],
      "metadata": {
        "id": "UOr7Jqvri2CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* An ensemble technique in machine learning is a method of combining multiple models to improve the accuracy and robustness of a predictive system. Rather than relying on a single model to make predictions, ensemble methods use a combination of models that are trained on the same or different datasets to produce a more accurate and reliable prediction. Examples of ensemble techniques include bagging, boosting, and stacking"
      ],
      "metadata": {
        "id": "3WG-1YvSlwXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q2`. Why are ensemble techniques used in machine learning?\n"
      ],
      "metadata": {
        "id": "dxst1RRPjRto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ensemble techniques are used in machine learning to improve the accuracy and robustness of predictive models. By combining multiple models, ensemble methods can reduce the risk of overfitting, increase generalization performance, and improve the stability of the predictions. \n",
        "* Ensemble techniques can also help to handle complex and noisy datasets, where individual models may struggle to capture all the patterns and relationships in the data. Additionally, ensemble methods can be used to mitigate the biases and limitations of individual models and can provide a more comprehensive and reliable prediction."
      ],
      "metadata": {
        "id": "T-YOkkkUl7pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q3`. What is bagging?\n"
      ],
      "metadata": {
        "id": "ysvjPAJTjTeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Bagging, short for Bootstrap Aggregation, is an ensemble technique in machine learning where multiple models are trained on different random subsets of the training data, with replacement. \n",
        "* Each model is trained independently on its own subset of the data and produces a prediction. The final prediction is then obtained by aggregating the predictions of all the models, either by taking the average for regression problems or the majority vote for classification problems. \n",
        "* Bagging can improve the performance of a single model by reducing the variance and improving the stability of the predictions, especially when the individual models are prone to overfitting."
      ],
      "metadata": {
        "id": "ZcifE7j2mEii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q4`. What is boosting?\n"
      ],
      "metadata": {
        "id": "3S1uGPXujU3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Boosting is an ensemble technique in machine learning where multiple weak models are trained sequentially, with each model trying to improve the errors of the previous model. \n",
        "* The weak models are typically simple models such as decision trees with limited depth or small subsets of features. Boosting works by assigning higher weights to the misclassified samples in each iteration and adjusting the model to better fit those samples. \n",
        "* The final prediction is obtained by combining the predictions of all the weak models, weighted by their performance. Boosting can improve the performance of a single model by reducing the bias and improving the accuracy of the predictions, especially when the individual models are not strong enough to capture all the patterns and relationships in the data."
      ],
      "metadata": {
        "id": "Sbq5ZrfymxN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q5`. What are the benefits of using ensemble techniques?\n"
      ],
      "metadata": {
        "id": "FZTGTa3rjWA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The benefits of using ensemble techniques in machine learning include:\n",
        "\n",
        "1. **Improved accuracy:** Ensemble techniques can improve the accuracy of predictive models by reducing the variance and bias of the individual models and by combining the strengths of different models.\n",
        "\n",
        "2. **Better generalization:** Ensemble techniques can improve the generalization performance of predictive models by reducing overfitting and increasing the stability of the predictions.\n",
        "\n",
        "3. **Handling complex datasets:** Ensemble techniques can handle complex and noisy datasets by combining the results of multiple models that capture different patterns and relationships in the data.\n",
        "\n",
        "4. **Robustness:** Ensemble techniques can provide a more robust prediction by mitigating the biases and limitations of individual models and by reducing the impact of outliers and errors in the data.\n",
        "\n",
        "5. **Flexibility:** Ensemble techniques can be applied to different types of models and can be customized to fit different types of problems and data"
      ],
      "metadata": {
        "id": "Er5ox5b9nBOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q6`. Are ensemble techniques always better than individual models?\n"
      ],
      "metadata": {
        "id": "ks2OLCr3jXHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* No, ensemble techniques are not always better than individual models. Ensemble techniques can improve the performance of a single model, but only if the individual models are weak or prone to overfitting. If the individual models are already accurate and robust, ensemble techniques may not provide significant improvements or may even lead to overfitting. Additionally, ensemble techniques may require more computational resources and may be more complex to implement and interpret than individual models. Therefore, the decision to use ensemble techniques should be based on the specific problem and data, and should be evaluated carefully using appropriate metrics and validation techniques."
      ],
      "metadata": {
        "id": "jDW4NgnenWtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q7`. How is the confidence interval calculated using bootstrap?\n"
      ],
      "metadata": {
        "id": "f2zj1fMCjYpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To calculate the confidence interval using bootstrap, we follow these steps:\n",
        "\n",
        "  1. Generate multiple bootstrap samples by randomly sampling the original dataset with replacement.\n",
        "\n",
        "  2. For each bootstrap sample, calculate the sample statistic of interest (e.g., mean, median, standard deviation).\n",
        "\n",
        "  3. Calculate the standard error of the statistic by computing the standard deviation of the bootstrap statistics across all the samples.\n",
        "\n",
        "  4. Use the standard error to compute the confidence interval, which is typically defined as the sample statistic plus or minus a multiple of the standard error, where the multiple is determined by the desired level of confidence (e.g., 95%, 99%).\n",
        "\n",
        "* The confidence interval provides a range of values that is likely to contain the true population parameter with a certain degree of confidence. The width of the interval depends on the variability of the data and the sample size, with larger samples and less variability leading to narrower intervals."
      ],
      "metadata": {
        "id": "SR7JFdqlniU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q8`. How does bootstrap work and What are the steps involved in bootstrap?\n"
      ],
      "metadata": {
        "id": "pfXu5GWdjaG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Bootstrap is a resampling technique used in statistics and machine learning to estimate the variability and uncertainty of a statistic or model parameter. Here are the steps involved in bootstrap:\n",
        "\n",
        "  1. Take a random sample of size N with replacement from the original dataset of size N.\n",
        "\n",
        "  2. Calculate the statistic of interest (e.g., mean, median, standard deviation) on the bootstrap sample.\n",
        "\n",
        "  3. Repeat steps 1 and 2 B times to obtain B bootstrap statistics.\n",
        "\n",
        "  4. Calculate the standard error of the bootstrap statistics, which is an estimate of the standard deviation of the sampling distribution of the statistic.\n",
        "\n",
        "  5. Construct the confidence interval by finding the values of the bootstrap statistic that fall within the desired percentile range (e.g., 95%, 99%).\n",
        "\n",
        "* Bootstrap works by simulating multiple samples from the original dataset, each of size N, and calculating the statistic of interest on each sample. By repeating this process many times, we can obtain a distribution of the statistic and estimate its variability and uncertainty. Bootstrap is useful when analytical methods for estimating the variability of a statistic or parameter are not available or are too complex to apply."
      ],
      "metadata": {
        "id": "DGWwGoyyn3qG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q9`. A researcher wants to estimate the mean height of a population of trees. They measure the height of asample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
      ],
      "metadata": {
        "id": "fxOxIs0rjbQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# original sample data\n",
        "data = np.array([15.3, 16.1, 13.8, 14.2, 16.5, 13.4, 15.2, 15.7, 14.5, 16.2,\n",
        "                14.3, 13.9, 15.1, 14.7, 16.8, 14.8, 15.9, 14.6, 15.0, 13.7,\n",
        "                16.0, 15.5, 14.9, 16.1, 15.6, 13.5, 14.0, 16.3, 15.8, 14.4,\n",
        "                15.3, 13.8, 14.2, 16.5, 13.4, 15.2, 15.7, 14.5, 16.2, 14.3,\n",
        "                13.9, 15.1, 14.7, 16.8, 14.8, 15.9, 14.6, 15.0, 13.7, 16.0])"
      ],
      "metadata": {
        "id": "bRu6yBtjk-aW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 10000\n",
        "\n",
        "bootstrap_samples = np.random.choice(data, size=(n_samples, 50), replace=True)\n",
        "bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
        "\n",
        "mean_of_means = np.mean(bootstrap_means)\n",
        "std_error_of_means = np.std(bootstrap_means) / np.sqrt(n_samples)"
      ],
      "metadata": {
        "id": "7lYFKXj5lAQr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate confidence interval for population mean\n",
        "lower_bound = mean_of_means - 1.96 * std_error_of_means\n",
        "upper_bound = mean_of_means + 1.96 * std_error_of_means\n",
        "\n",
        "print(\"95% confidence interval for population mean height: [{:.2f}, {:.2f}]\".format(lower_bound, upper_bound))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-pFMKDVlRlc",
        "outputId": "b07cf35e-90f7-485c-bf0d-632b77803c8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% confidence interval for population mean height: [15.02, 15.03]\n"
          ]
        }
      ]
    }
  ]
}